#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨
#           This file was automatically generated from src/transformers/models/gemma/modular_gemma.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_gemma.py file directly. One of our CI enforces this.
#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨
# coding=utf-8
# Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
from shutil import copyfile
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

import sentencepiece as spm

from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer
from transformers.utils import logging


if TYPE_CHECKING:
    from ...tokenization_utils_base import TextInput

logger = logging.get_logger(__name__)

VOCAB_FILES_NAMES = {"vocab_file": "tokenizer.model"}

SPIECE_UNDERLINE = "▁"


class ModernGPT2Tokenizer(PreTrainedTokenizer):
    """
    Construct a ModernGPT2 tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is
    no padding token in the original model.

    Args:
        vocab_file (`str`):
            Path to the vocabulary file.
        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<unk>"`):
            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
            token instead.
        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<bos>"`):
            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.
        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<eos>"`):
            The end of sequence token.
        pad_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `"<pad>"`):
            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
            attention mechanisms or loss computation.
        sp_model_kwargs (`Dict[str, Any]`, `Optional`, *optional*):
            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for
            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,
            to set:

            - `enable_sampling`: Enable subword regularization.
            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.

              - `nbest_size = {0,1}`: No sampling is performed.
              - `nbest_size > 1`: samples from the nbest_size results.
              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
                using forward-filtering-and-backward-sampling algorithm.

            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
              BPE-dropout.

        add_bos_token (`bool`, *optional*, defaults to `True`):
            Whether or not to add an `bos_token` at the start of sequences.
        add_eos_token (`bool`, *optional*, defaults to `False`):
            Whether or not to add an `eos_token` at the end of sequences.
        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like
            extra spaces.
        use_default_system_prompt (`bool`, *optional*, defaults to `False`): # MODIFIED: This might be specific to Gemma and may need to be removed or adapted for ModernGPT2
            Whether or not the default system prompt for ModernGPT2 should be used.
        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):
            Whether or not to add spaces between special tokens.
    """

    vocab_files_names = VOCAB_FILES_NAMES
    model_input_names = ["input_ids", "attention_mask"]
    
    # BitBPE: Define prefix tokens for bit redistribution
    _bitbpe_prefix_tokens = None  # Will be initialized if BitBPE is enabled
    _bitbpe_9bit_tokens = None  # Mapping from 9-bit values to token IDs
    _bitbpe_enabled = False
    _bitbpe_9bit_start_id = None  # Starting ID for 9-bit tokens
    _actual_vocab_size = None  # Actual vocab size including BitBPE tokens

    def __init__(
        self,
        vocab_file,
        unk_token="<unk>",
        bos_token="<bos>",
        eos_token="<eos>",
        pad_token="<pad>",
        sp_model_kwargs: Optional[Dict[str, Any]] = None,
        add_bos_token=True,
        add_eos_token=False,
        clean_up_tokenization_spaces=False,
        use_default_system_prompt=False,
        spaces_between_special_tokens=False,
        tokenizer_type: str = "bpe",  # BitBPE: Added tokenizer type parameter
        bitbpe_config: Optional[Dict[str, Any]] = None,  # BitBPE: Configuration dict
        **kwargs,
    ):
        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs
        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token
        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token
        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token
        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token

        self.vocab_file = vocab_file
        self.add_bos_token = add_bos_token
        self.add_eos_token = add_eos_token
        self.use_default_system_prompt = use_default_system_prompt
        self.tokenizer_type = tokenizer_type  # BitBPE: Store tokenizer type
        self.bitbpe_config = bitbpe_config or {}  # BitBPE: Store configuration
        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)
        self.sp_model.Load(vocab_file)
        
        # BitBPE: Initialize BitBPE if enabled
        if self.tokenizer_type == "bitbpe":
            self._initialize_bitbpe()

        super().__init__(
            bos_token=bos_token,
            eos_token=eos_token,
            unk_token=unk_token,
            pad_token=pad_token,
            add_bos_token=add_bos_token,
            add_eos_token=add_eos_token,
            sp_model_kwargs=sp_model_kwargs,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            use_default_system_prompt=use_default_system_prompt,
            spaces_between_special_tokens=spaces_between_special_tokens,
            tokenizer_type=tokenizer_type,
            bitbpe_config=bitbpe_config,
            **kwargs,
        )

    def __getstate__(self):
        state = self.__dict__.copy()
        state["sp_model"] = None
        state["sp_model_proto"] = self.sp_model.serialized_model_proto()
        return state

    def __setstate__(self, d):
        self.__dict__.update(d)
        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)
        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)

    @property
    def vocab_size(self):
        """Returns actual vocab size including BitBPE tokens."""
        if self._bitbpe_enabled and self._actual_vocab_size is not None:
            return self._actual_vocab_size
        return self.sp_model.get_piece_size()

    def get_vocab(self):
        """Returns vocab as a dict including BitBPE tokens."""
        # Use base vocab size for regular tokens
        base_vocab_size = self.sp_model.get_piece_size()
        vocab = {self.convert_ids_to_tokens(i): i for i in range(base_vocab_size)}
        vocab.update(self.added_tokens_encoder)
        
        if self._bitbpe_enabled:
            # Add prefix tokens
            for prefix_6bit, token_id in self._bitbpe_prefix_tokens.items():
                vocab[f"<PREFIX_{prefix_6bit:02X}>"] = token_id
            
            # Add 9-bit tokens
            if self._bitbpe_9bit_tokens:
                for value_9bit, token_id in self._bitbpe_9bit_tokens.items():
                    vocab[f"<9BIT_{value_9bit:03X}>"] = token_id
        
        return vocab
    
    def set_tokenizer_type(self, tokenizer_type: str):
        """BitBPE: Set the tokenizer type and reinitialize if needed."""
        if tokenizer_type not in ["bpe", "bitbpe"]:
            raise ValueError(f"Unsupported tokenizer type: {tokenizer_type}")
        self.tokenizer_type = tokenizer_type
        if tokenizer_type == "bitbpe":
            self._initialize_bitbpe()
        else:
            self._bitbpe_enabled = False
            self._bitbpe_prefix_tokens = None
            self._bitbpe_9bit_tokens = None
            self._actual_vocab_size = None
    
    def _initialize_bitbpe(self):
        """BitBPE: Initialize BitBPE prefix tokens and 9-bit tokens for UTF-8 bit redistribution."""
        self._bitbpe_enabled = True
        
        # BitBPE: Get configuration from bitbpe_config if available
        base_vocab_size = self.sp_model.get_piece_size()
        if self.bitbpe_config:
            num_prefix_tokens = self.bitbpe_config.get('num_prefix_tokens', 16)
            prefix_token_start_id = self.bitbpe_config.get('prefix_token_start_id', base_vocab_size + 2)
        else:
            num_prefix_tokens = 16
            prefix_token_start_id = base_vocab_size + 2
        
        # BitBPE: Reserve tokens for 6-bit prefixes
        # These represent the first 6 bits of UTF-8 encoded bytes
        self._bitbpe_prefix_tokens = {}
        
        # BitBPE: Create mapping for 3-byte UTF-8 prefixes
        # For 3-byte sequences starting with 0xE0-0xEF (11100000-11101111)
        # The top 6 bits of these bytes range from 111000 to 111011
        for i in range(num_prefix_tokens):  # 16 prefixes for 0xE0 to 0xEF
            # Calculate the actual byte value (0xE0 + i)
            byte_val = 0xE0 + i
            # Extract top 6 bits
            prefix_6bit = byte_val >> 2
            # Map to special tokens starting from prefix_token_start_id
            prefix_token_id = prefix_token_start_id + i
            self._bitbpe_prefix_tokens[prefix_6bit] = prefix_token_id
        
        # Initialize 9-bit tokens (512 tokens for values 0-511)
        self._bitbpe_9bit_start_id = prefix_token_start_id + num_prefix_tokens
        self._bitbpe_9bit_tokens = {}
        self._bitbpe_9bit_reverse = {}  # For decoding
        
        for i in range(512):  # 9-bit values: 0-511
            token_id = self._bitbpe_9bit_start_id + i
            self._bitbpe_9bit_tokens[i] = token_id
            self._bitbpe_9bit_reverse[token_id] = i
        
        # Calculate actual vocabulary size - need to account for the highest token ID
        # The vocab size should be the highest token ID + 1
        max_token_id = self._bitbpe_9bit_start_id + 511  # Last 9-bit token
        self._actual_vocab_size = max_token_id + 1
        
        logger.info(f"BitBPE initialized:")
        logger.info(f"  - Base vocab size: {base_vocab_size}")
        logger.info(f"  - Prefix tokens: {num_prefix_tokens} (IDs {prefix_token_start_id}-{prefix_token_start_id + num_prefix_tokens - 1})")
        logger.info(f"  - 9-bit tokens: 512 (IDs {self._bitbpe_9bit_start_id}-{self._bitbpe_9bit_start_id + 511})")
        logger.info(f"  - Total vocab size: {self._actual_vocab_size}")
    
    def _encode_bitbpe(self, text: str) -> List[int]:
        """BitBPE: Encode text, applying bit redistribution only to byte fallback sequences."""
        # First, get regular BPE encoding
        regular_tokens = self.sp_model.encode(text)
        
        if not self._bitbpe_enabled or not self._bitbpe_prefix_tokens:
            return regular_tokens
        
        # Convert token IDs to token strings to identify byte tokens
        result_tokens = []
        i = 0
        last_prefix = None  # Track last prefix for deduplication
        
        while i < len(regular_tokens):
            token_id = regular_tokens[i]
            token_str = self.sp_model.IdToPiece(token_id)
            
            # Check if this is a byte token (format: <0xXX>)
            if token_str.startswith('<0x') and token_str.endswith('>'):
                # This is a byte token - check if it's part of a 3-byte UTF-8 sequence
                # Extract the byte value
                byte_val = int(token_str[3:-1], 16)
                
                # Check if this starts a 3-byte UTF-8 sequence (0xE0-0xEF)
                if 0xE0 <= byte_val <= 0xEF and i + 2 < len(regular_tokens):
                    # Check if next two tokens are also byte tokens
                    token2_str = self.sp_model.IdToPiece(regular_tokens[i + 1])
                    token3_str = self.sp_model.IdToPiece(regular_tokens[i + 2])
                    
                    if (token2_str.startswith('<0x') and token2_str.endswith('>') and
                        token3_str.startswith('<0x') and token3_str.endswith('>')):
                        # We have a 3-byte sequence - apply BitBPE
                        b1 = byte_val
                        b2 = int(token2_str[3:-1], 16)
                        b3 = int(token3_str[3:-1], 16)
                        
                        # Extract 6-bit prefix from first byte
                        prefix_6bit = b1 >> 2  # Top 6 bits
                        
                        # Check if we need to add the prefix token (deduplication)
                        if prefix_6bit != last_prefix:
                            # Prefix changed, add the prefix token
                            if prefix_6bit in self._bitbpe_prefix_tokens:
                                result_tokens.append(self._bitbpe_prefix_tokens[prefix_6bit])
                                last_prefix = prefix_6bit
                        
                        # Redistribute remaining bits into two 9-bit tokens
                        # b1's lower 2 bits + b2's upper 7 bits = 9 bits
                        b2_hat = ((b1 & 0x03) << 7) | ((b2 & 0xFE) >> 1)
                        # b2's lower 1 bit + b3's 8 bits = 9 bits
                        b3_hat = ((b2 & 0x01) << 8) | b3
                        
                        # Map 9-bit values to token IDs
                        if b2_hat in self._bitbpe_9bit_tokens and b3_hat in self._bitbpe_9bit_tokens:
                            result_tokens.append(self._bitbpe_9bit_tokens[b2_hat])
                            result_tokens.append(self._bitbpe_9bit_tokens[b3_hat])
                        else:
                            # Fallback if 9-bit mapping fails (shouldn't happen)
                            result_tokens.extend([regular_tokens[i], regular_tokens[i+1], regular_tokens[i+2]])
                            logger.warning(f"BitBPE: Failed to map 9-bit values {b2_hat}, {b3_hat}")
                        
                        # Skip the next two tokens
                        i += 3
                        continue
            else:
                # Not a byte token - reset prefix tracking
                last_prefix = None
            
            # Regular token - just add it
            result_tokens.append(token_id)
            i += 1
        
        return result_tokens

    def tokenize(self, text: "TextInput", **kwargs) -> List[str]:
        """
        Args:
            text: TextInput
        Returns tokenized strings with BitBPE tokens if enabled.
        """
        if self.tokenizer_type == "bitbpe" and self._bitbpe_enabled:
            # First encode with BitBPE to get token IDs
            token_ids = self.encode(text, add_special_tokens=False)
            
            # Convert token IDs back to strings
            result = []
            for tid in token_ids:
                if tid < self.sp_model.get_piece_size():
                    # Regular vocab token
                    token_str = self.sp_model.IdToPiece(tid)
                else:
                    # BitBPE special token
                    token_str = self._convert_id_to_token(tid)
                result.append(token_str)
            
            return result
        else:
            # Fall back to parent implementation for non-BitBPE
            return super().tokenize(text, **kwargs)

    def _tokenize(self, text, **kwargs):
        """
        Args:
            text: TextInput
        Returns a tokenized string. The ModernGPT2 tokenizer never adds a prefix space.
        """
        # Always use regular tokenization for _tokenize
        # BitBPE is handled in the encode() method
        return self.sp_model.encode(text, out_type=str)
    
    def _should_apply_bitbpe(self, token: str) -> bool:
        """BitBPE: Check if a token should have BitBPE applied."""
        # Check if token contains CJK characters that would benefit from BitBPE
        try:
            for char in token:
                if '\u4e00' <= char <= '\u9fff' or '\u3040' <= char <= '\u309f' or '\uac00' <= char <= '\ud7af':
                    return True
        except:
            pass
        return False

    def _convert_token_to_id_with_bitbpe(self, token):
        """Converts a token string to an id, including BitBPE tokens."""
        # First try regular vocab
        result = self.sp_model.piece_to_id(token)
        if result != self.unk_token_id:
            return result
        
        # Check if it's a BitBPE token
        if self._bitbpe_enabled:
            # Check prefix tokens
            if token.startswith("<PREFIX_") and token.endswith(">"):
                try:
                    prefix_val = int(token[8:-1], 16)
                    if prefix_val in self._bitbpe_prefix_tokens:
                        return self._bitbpe_prefix_tokens[prefix_val]
                except ValueError:
                    pass
            
            # Check 9-bit tokens
            if token.startswith("<9BIT_") and token.endswith(">"):
                try:
                    bit9_val = int(token[6:-1], 16)
                    if bit9_val in self._bitbpe_9bit_tokens:
                        return self._bitbpe_9bit_tokens[bit9_val]
                except ValueError:
                    pass
        
        return self.unk_token_id

    def _convert_token_to_id(self, token):
        """Converts a token (str) in an id using the vocab."""
        return self.sp_model.piece_to_id(token)

    def _convert_id_to_token(self, index):
        """Converts an index (integer) in a token (str) using the vocab."""
        # Check if it's in regular vocab
        if index < self.sp_model.get_piece_size():
            return self.sp_model.IdToPiece(index)
        
        # Check if it's a BitBPE token
        if self._bitbpe_enabled:
            # Check prefix tokens
            if self._bitbpe_prefix_tokens:
                for prefix_6bit, token_id in self._bitbpe_prefix_tokens.items():
                    if token_id == index:
                        return f"<PREFIX_{prefix_6bit:02X}>"
            
            # Check 9-bit tokens
            if self._bitbpe_9bit_reverse and index in self._bitbpe_9bit_reverse:
                value_9bit = self._bitbpe_9bit_reverse[index]
                return f"<9BIT_{value_9bit:03X}>"
        
        return self.unk_token

    def encode(
        self,
        text: str,
        text_pair: Optional[str] = None,
        add_special_tokens: bool = True,
        padding: bool = False,
        truncation: bool = False,
        max_length: Optional[int] = None,
        return_tensors: Optional[str] = None,
        **kwargs
    ) -> List[int]:
        """
        Encode text(s) to token IDs. Overrides parent to support BitBPE encoding.
        """
        # BitBPE: Use BitBPE encoding if enabled
        if self.tokenizer_type == "bitbpe" and self._bitbpe_enabled:
            # Handle text pair if provided
            if text_pair is not None:
                # Encode both texts with BitBPE
                tokens = self._encode_bitbpe(text)
                tokens_pair = self._encode_bitbpe(text_pair)
                
                # Build inputs with special tokens if needed
                if add_special_tokens:
                    tokens = self.build_inputs_with_special_tokens(tokens, tokens_pair)
                else:
                    tokens = tokens + tokens_pair
            else:
                # Single text encoding
                tokens = self._encode_bitbpe(text)
                
                # Add special tokens if needed
                if add_special_tokens:
                    tokens = self.build_inputs_with_special_tokens(tokens)
            
            # Handle padding, truncation etc. if needed
            # For now, return the tokens directly
            return tokens
        
        # Fall back to parent's encode method for regular BPE
        return super().encode(
            text,
            text_pair=text_pair,
            add_special_tokens=add_special_tokens,
            padding=padding,
            truncation=truncation,
            max_length=max_length,
            return_tensors=return_tensors,
            **kwargs
        )
    
    def _encode_plus(
        self,
        text: str,
        text_pair: Optional[str] = None,
        add_special_tokens: bool = True,
        padding: bool = False,
        truncation: bool = False,
        max_length: Optional[int] = None,
        stride: int = 0,
        is_split_into_words: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        return_tensors: Optional[str] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
        **kwargs
    ) -> dict:
        """
        Override _encode_plus to use BitBPE encoding when enabled.
        """
        if self.tokenizer_type == "bitbpe" and self._bitbpe_enabled:
            # Use our custom encode method which handles BitBPE
            input_ids = self.encode(
                text,
                text_pair=text_pair,
                add_special_tokens=add_special_tokens,
                padding=padding,
                truncation=truncation,
                max_length=max_length,
                return_tensors=None,
                **kwargs
            )
            
            # Build the output dictionary
            encoded_inputs = {"input_ids": input_ids}
            
            # Add attention mask if requested (default to all 1s)
            if return_attention_mask is not False:
                encoded_inputs["attention_mask"] = [1] * len(input_ids)
            
            # Handle padding if needed
            if padding and max_length:
                padding_length = max_length - len(input_ids)
                if padding_length > 0:
                    encoded_inputs["input_ids"] = encoded_inputs["input_ids"] + [self.pad_token_id] * padding_length
                    if "attention_mask" in encoded_inputs:
                        encoded_inputs["attention_mask"] = encoded_inputs["attention_mask"] + [0] * padding_length
            
            # Handle truncation if needed
            if truncation and max_length and len(encoded_inputs["input_ids"]) > max_length:
                encoded_inputs["input_ids"] = encoded_inputs["input_ids"][:max_length]
                if "attention_mask" in encoded_inputs:
                    encoded_inputs["attention_mask"] = encoded_inputs["attention_mask"][:max_length]
            
            # Convert to tensors if requested
            if return_tensors is not None:
                # This would normally use the convert_to_tensors method
                # For now, we'll let the parent class handle tensor conversion
                pass
            
            return encoded_inputs
        
        # Fall back to parent's _encode_plus for regular BPE
        return super()._encode_plus(
            text,
            text_pair=text_pair,
            add_special_tokens=add_special_tokens,
            padding=padding,
            truncation=truncation,
            max_length=max_length,
            stride=stride,
            is_split_into_words=is_split_into_words,
            pad_to_multiple_of=pad_to_multiple_of,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            return_overflowing_tokens=return_overflowing_tokens,
            return_special_tokens_mask=return_special_tokens_mask,
            return_offsets_mapping=return_offsets_mapping,
            return_length=return_length,
            verbose=verbose,
            **kwargs
        )
    
    def convert_tokens_to_string(self, tokens):
        """Converts a sequence of tokens (string) in a single string."""
        current_sub_tokens = []
        out_string = ""
        for token in tokens:
            # make sure that special tokens are not decoded using sentencepiece model
            if token in self._added_tokens_encoder:
                out_string += self.sp_model.decode(current_sub_tokens) + token
                current_sub_tokens = []
            else:
                current_sub_tokens.append(token)
        out_string += self.sp_model.decode(current_sub_tokens)
        return out_string

    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> Tuple[str]:
        """
        Save the vocabulary and special tokens file to a directory.

        Args:
            save_directory (`str`):
                The directory in which to save the vocabulary.

        Returns:
            `Tuple(str)`: Paths to the files saved.
        """
        if not os.path.isdir(save_directory):
            logger.error(f"Vocabulary path ({save_directory}) should be a directory")
            return
        out_vocab_file = os.path.join(
            save_directory, (filename_prefix + "-" if filename_prefix else "") + VOCAB_FILES_NAMES["vocab_file"]
        )

        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):
            copyfile(self.vocab_file, out_vocab_file)
        elif not os.path.isfile(self.vocab_file):
            with open(out_vocab_file, "wb") as fi:
                content_spiece_model = self.sp_model.serialized_model_proto()
                fi.write(content_spiece_model)

        return (out_vocab_file,)

    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
        bos_token_id = [self.bos_token_id] if self.add_bos_token else []
        eos_token_id = [self.eos_token_id] if self.add_eos_token else []

        output = bos_token_id + token_ids_0 + eos_token_id

        if token_ids_1 is not None:
            output = output + bos_token_id + token_ids_1 + eos_token_id

        return output

    def get_special_tokens_mask(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False
    ) -> List[int]:
        """
        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
        special tokens using the tokenizer `prepare_for_model` method.

        Args:
            token_ids_0 (`List[int]`):
                List of IDs.
            token_ids_1 (`List[int]`, *optional*):
                Optional second list of IDs for sequence pairs.
            already_has_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not the token list is already formatted with special tokens for the model.

        Returns:
            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
        """
        if already_has_special_tokens:
            return super().get_special_tokens_mask(
                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True
            )

        bos_token_id = [1] if self.add_bos_token else []
        eos_token_id = [1] if self.add_eos_token else []

        if token_ids_1 is None:
            return bos_token_id + ([0] * len(token_ids_0)) + eos_token_id
        return (
            bos_token_id
            + ([0] * len(token_ids_0))
            + eos_token_id
            + bos_token_id
            + ([0] * len(token_ids_1))
            + eos_token_id
        )

    def create_token_type_ids_from_sequences(
        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -> List[int]:
        """
        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT
        sequence pair mask has the following format:

        ```
        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
        | first sequence    | second sequence |
        ```

        if token_ids_1 is None, only returns the first portion of the mask (0s).

        Args:
            token_ids_0 (`List[int]`):
                List of ids.
            token_ids_1 (`List[int]`, *optional*):
                Optional second list of IDs for sequence pairs.

        Returns:
            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).
        """
        bos_token_id = [self.bos_token_id] if self.add_bos_token else []
        eos_token_id = [self.eos_token_id] if self.add_eos_token else []

        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)

        if token_ids_1 is not None:
            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)

        return output

    def _decode(
        self,
        token_ids: List[int],
        skip_special_tokens: bool = False,
        spaces_between_special_tokens: bool = False,
        **kwargs,
    ) -> str:
        # BitBPE: Handle BitBPE decoding if enabled
        if self.tokenizer_type == "bitbpe" and self._bitbpe_enabled:
            return self._decode_bitbpe(token_ids, skip_special_tokens, spaces_between_special_tokens)
        
        sub_texts = []
        current_sub_text = []
        for ids in token_ids:
            if skip_special_tokens and ids in self.all_special_ids:
                continue
            if ids in self._added_tokens_decoder:
                if current_sub_text:
                    sub_texts.append(self.sp_model.decode(current_sub_text))
                sub_texts.append(self._added_tokens_decoder[ids].content)
                current_sub_text = []
            else:
                current_sub_text.append(ids)
        if current_sub_text:
            sub_texts.append(self.sp_model.decode(current_sub_text))

        if spaces_between_special_tokens:
            sub_texts = " ".join(sub_texts)
        else:
            sub_texts = "".join(sub_texts)

        return sub_texts.replace(SPIECE_UNDERLINE, " ")
    
    def _decode_bitbpe(self, token_ids: List[int], skip_special_tokens: bool, spaces_between_special_tokens: bool) -> str:
        """BitBPE: Decode token IDs that may contain BitBPE encoded sequences."""
        if not self._bitbpe_enabled or not self._bitbpe_9bit_reverse:
            # Fallback to regular decoding
            return self._decode(
                token_ids,
                skip_special_tokens=skip_special_tokens,
                spaces_between_special_tokens=spaces_between_special_tokens,
            )
        
        # Process tokens to handle BitBPE sequences
        processed_tokens = []
        i = 0
        current_prefix = None
        
        while i < len(token_ids):
            token_id = token_ids[i]
            
            # Skip special tokens if requested
            if skip_special_tokens and token_id in self.all_special_ids:
                i += 1
                continue
            
            # Check if this is a BitBPE prefix token
            is_prefix = False
            if self._bitbpe_prefix_tokens:
                for prefix_6bit, prefix_id in self._bitbpe_prefix_tokens.items():
                    if prefix_id == token_id:
                        current_prefix = prefix_6bit
                        is_prefix = True
                        i += 1
                        break
            
            if is_prefix:
                continue
            
            # Check if this is a 9-bit token
            if token_id in self._bitbpe_9bit_reverse and i + 1 < len(token_ids):
                next_token_id = token_ids[i + 1]
                if next_token_id in self._bitbpe_9bit_reverse and current_prefix is not None:
                    # We have a complete BitBPE sequence
                    b2_hat = self._bitbpe_9bit_reverse[token_id]
                    b3_hat = self._bitbpe_9bit_reverse[next_token_id]
                    
                    # Reconstruct the original 3 bytes
                    b1 = (current_prefix << 2) | ((b2_hat >> 7) & 0x03)
                    b2 = ((b2_hat & 0x7F) << 1) | ((b3_hat >> 8) & 0x01)
                    b3 = b3_hat & 0xFF
                    
                    # Convert bytes back to token IDs
                    for byte_val in [b1, b2, b3]:
                        byte_token = f"<0x{byte_val:02X}>"
                        byte_token_id = self.sp_model.PieceToId(byte_token)
                        if byte_token_id != self.unk_token_id:
                            processed_tokens.append(byte_token_id)
                    
                    i += 2
                    continue
            
            # Not part of a BitBPE sequence - add token as is if it's in regular vocab
            if token_id < self.sp_model.get_piece_size():
                processed_tokens.append(token_id)
            i += 1
        
        # Now decode the processed tokens using the parent class method
        # to avoid recursion
        sub_texts = []
        current_sub_text = []
        for ids in processed_tokens:
            if skip_special_tokens and ids in self.all_special_ids:
                continue
            if ids in self._added_tokens_decoder:
                if current_sub_text:
                    sub_texts.append(self.sp_model.decode(current_sub_text))
                sub_texts.append(self._added_tokens_decoder[ids].content)
                current_sub_text = []
            else:
                current_sub_text.append(ids)
        if current_sub_text:
            sub_texts.append(self.sp_model.decode(current_sub_text))

        if spaces_between_special_tokens:
            sub_texts = " ".join(sub_texts)
        else:
            sub_texts = "".join(sub_texts)

        return sub_texts.replace(SPIECE_UNDERLINE, " ")


__all__ = ["ModernGPT2Tokenizer"]
